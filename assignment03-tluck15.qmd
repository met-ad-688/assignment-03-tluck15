---
title: Assignment 03
author:
  - name: Taylor Luckenbill
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2025-09-22'

format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2
  docx:
    toc: true   # remove any reference-doc line if you don't have a template

date-modified: today
date-format: long
execute:
  echo: true
  eval: true
  freeze: auto
jupyter: python3
---


```{python}
#|echo: false
import pandas as pd
import plotly.express as px
import plotly.io as pio
from pyspark.sql import SparkSession
import re
import numpy as np
import plotly.graph_objects as go
from pyspark.sql.functions import col, split, explode, regexp_replace, transform, when
from pyspark.sql import functions as F
from pyspark.sql.functions import col, monotonically_increasing_id
```

```{python}
#|echo: false
np.random.seed(2)

pio.renderers.default = "notebook"

# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("data/lightcast_job_postings.csv")
df.createOrReplaceTempView("job_postings")

# Show Schema and Sample Data
#print("---This is Diagnostic check, No need to print it in the final doc---")

#df.printSchema() # comment this line when rendering the submission
df.show(5)
```
# Analysis Objective
## Data Preparation Activities
In order to complete the analysis the first step is to load and prepare the data. For this analysis I will update data types, calculate average salary, remove extraneous characters from relevant data text, and replace null values.

For the data set job_postings save as df and change the columns SALARY_FROM, SALARY_TO, SALARY, MIN_YEARS_EXPERIENCE, and MAX_YEARS_EXPERIENCE to type float.
```{python}
df = (
    df.withColumn("SALARY_FROM", col("SALARY_FROM").cast("float"))
      .withColumn("SALARY_TO", col("SALARY_TO").cast("float"))
      .withColumn("SALARY", col("SALARY").cast("float"))
      .withColumn("MIN_YEARS_EXPERIENCE", col("MIN_YEARS_EXPERIENCE").cast("float"))
      .withColumn("MAX_YEARS_EXPERIENCE", col("MAX_YEARS_EXPERIENCE").cast("float"))
)
```
Next the median needs to be calculated as it will be used to fill na values. Create a new method called compute_median. 
```{python}
def compute_median(sdf,col_name):
    q = sdf.approxQuantile(col_name,[0.5],0.01)
    return q[0] if q else None

median_from = compute_median(df,"SALARY_FROM")
median_to = compute_median(df, "SALARY_TO")
median_salary = compute_median(df, "SALARY")
```
```{python}
#|echo: false
print("medians: ", median_from, median_to, median_salary)
```
Next fill the missing values in the SALARY_FROM, SALARY_TO, SALARY with the median values.
```{python}
df = df.fillna({
    "SALARY_FROM": median_from,
    "SALARY_TO": median_to,
    "SALARY": median_salary})
```
Calculate average salary by adding the min and max salaries and dividing by 2.
```{python}
df = df.withColumn("Average Salary",(col("SALARY_FROM")+col("SALARY_TO"))/2)
```

```{python}
#|echo: false
print(df.columns) 
```
Next remove extraneous characters from education level names.
```{python}
from pyspark.sql.functions import regexp_replace, col

df = df.withColumn(
    "EDUCATION_LEVELS_NAME",
    regexp_replace(col("EDUCATION_LEVELS_NAME"), "[\n\r]", "")  
)
```
## Create a box and whisker plot to analyze variation across occupations.
### Begin with more data prep
Select the columns to maintain in the analysis
```{python}
export_cols = [
    "EDUCATION_LEVELS_NAME",
    "REMOTE_TYPE_NAME",
    "MAX_YEARS_EXPERIENCE",
    "Average Salary",
    "LOT_V6_SPECIALIZED_OCCUPATION_NAME"
]


```
`confirmed selected columns`
```{python}
df_selected = df.select(export_cols)
df_selected.show(3)
```
Transform pyspark data into pandas dataframe from visualization.
```{python}
pdf = df_selected.toPandas()
pdf.to_csv("./data/lighthouse_cleaned.csv", index=False)
```
```{python}
#|echo: false
print(len(pdf))
```
Compute median salaries by group and sort the groups decending on median salaries.
```{python}
median_salaries = pdf.groupby("LOT_V6_SPECIALIZED_OCCUPATION_NAME")["Average Salary"].median()
```

```{python}
sorted_employment_types = median_salaries.sort_values(ascending=False).index
```
Make occupation a category and maintain group sorting.
```{python}
pdf["LOT_V6_SPECIALIZED_OCCUPATION_NAME"] = pd.Categorical(
    pdf["LOT_V6_SPECIALIZED_OCCUPATION_NAME"],
    categories=sorted_employment_types,
    ordered=True
)
```
### Variations in average salary by occupation name
The occupations with the most variance look to include SAP Analyst/Admin, Data Analyst, and Oracle consultant/analyst.I think a  general but not definitive trend is that the occupations with higher variance seem to be those that are more generalized. For example SAP Analyst/Admin covers a broad spectrum of skills related to SAP from analyst to admin whereas healthcare analyst has less observable variance and is specific in both its role analyst only and the sector of work healthcare.
```{python}
fig = px.box(
    pdf,
    x="LOT_V6_SPECIALIZED_OCCUPATION_NAME",
    y="Average Salary"
)
```

```{python}
#|echo: false
fig.show()
```

```{python}
#|echo: false
print(df.columns) 
```

```{python}
## Differences in salary by occupation
#|echo: false
from pyspark.sql.functions import lit
```
## Create a scatterplot to review salary differences across occupation
### begin with data prep
Create new column and assign 1 to each row so that it can be use for a record count by occupation later.
```{python}
df = df.withColumn("counter", lit(1))
```
Select the fields needed for the analysis.
```{python}
#parse
export_cols2 = [
    "LOT_V6_SPECIALIZED_OCCUPATION_NAME",
    "Average Salary",
    "counter"
]
```

```{python}
#|echo: false
df_selected2 = df.select(export_cols2)
#df_selected2.show(40)
```
Trannsform pyspark data to pandas dataframe for visualization.
```{python}
pdf2 = df_selected2.toPandas()
#pdf2.to_csv("./data/lighthouse_cleaned.csv", index=False)
#print(len(pdf2))
#pdf2.head(30)
```
Group the median average salaries by occupation and sum the counts of each occupation. 
```{python}
median_salaries2 = pdf2.groupby("LOT_V6_SPECIALIZED_OCCUPATION_NAME").agg({
    "Average Salary": "median",
    "counter": "sum"
}).reset_index()
```

```{python}
#|echo: false
#sorted_employment_types2 = median_salaries2.sort_values(ascending=False).index
median_salaries2.head(3)
```
### Median average salaries by occupation with number of job postings as the size parameter
The visual reveals the relative size differences of the job market. It is clear that data analyst is the biggest job pool and niche postings like marketing and healthcare analyst. The visual does not reveal differences in the salary ranges. This is due to the way missing values using the median.
```{python}
#|echo: false
fig2 = px.scatter(
    median_salaries2,
    x="LOT_V6_SPECIALIZED_OCCUPATION_NAME",
    y="Average Salary",
    size="counter",
    hover_name="LOT_V6_SPECIALIZED_OCCUPATION_NAME", 
    size_max=60       
)

fig2.update_layout(
    xaxis_title="Occupation",
    yaxis_title="Median Salary",
    title="Bubble Chart of Jobs: Median Salary vs. Occupation (Bubble = # Postings)",
    xaxis_tickangle=90              # rotate labels if long
)
```
## visualizing salary by education
### begin with data prep
Group the data in MIN_EDULEVELS_NAME into five groups: Associates or Lower, Bachelors, Masters, Professional, Other.
```{python}
df = df.withColumn(
    "education_group",
    F.when(F.col("MIN_EDULEVELS_NAME").isin("GED", "Associate", "No Education Listed"), "Associate's or lower")
     .when(F.col("MIN_EDULEVELS_NAME") == "Bachelor's degree", "Bachelor's")
     .when(F.col("MIN_EDULEVELS_NAME").isin("Master's degree"), "Master's")
     .when(F.col("MIN_EDULEVELS_NAME").isin("PhD", "Doctorate", "professional degree"), "PhD")
     .otherwise("Other")  # optional catch-all for unexpected values
)

```
```{python}
#|echo: false
df.show(3)
```
Extract columns needed for the analysis.
```{python}
#parse
export_cols3 = [
    "education_group",
    "Average Salary",
    "MAX_YEARS_EXPERIENCE"
]
```

```{python}
#|echo: false
df_selected3 = df.select(export_cols3)
#df_selected3.show(10)
```

```{python}
#|echo: false
pdf3 = df_selected3.toPandas()
#pdf2.to_csv("./data/lighthouse_cleaned.csv", index=False)
#print(len(pdf2))
pdf3.head(10)
```
Fill missing values with 0 if max years experience is empty.
```{python}
pdf3["MAX_YEARS_EXPERIENCE"] = pdf3["MAX_YEARS_EXPERIENCE"].fillna(0)

```
```{python}
#|echo: false
pdf3.head(10)
```

```{python}
#|echo: false
import matplotlib.pyplot as plt
```
The visual below looks at the relationship between minimum years experience, average salary, and education group. The first thing that is visible is that the most of jobs require 0-1 years of minimum experience. The callout here is that any null values were replaced with 0. However, I think this general trend aligns with what is observable on any job posting website. The nature of an organization suggests a n arrowing of positions as the requirements expand (year experience). Irrespective of whether you have no degree or a master's degree, there will always be a need for a first job. The other interesting observation is that jobs with high degree requirements tend to be the jobs with the most minimum years experience requirements. There also is an observable upward salary trend line as the minimum years experience gets larger and the variance in salary becomes tighter.
```{python}
#|echo: false
plt.figure(figsize=(10,6))

# group by education_group instead of max years experience
for edu, subset in pdf3.groupby("education_group"):
    # avoid overlapping points with jitter
    x_jitter = subset["MAX_YEARS_EXPERIENCE"] + np.random.normal(0, 0.2, size=len(subset))
    y_jitter = subset["Average Salary"] + np.random.normal(0, 0.2, size=len(subset))
    
    plt.scatter(x_jitter, y_jitter, alpha=0.6, label=edu)

plt.xlabel("Minimum Years Experience")
plt.ylabel("Average Salary")
plt.title("Salaries by Years Experience and Education Group")
plt.legend(title="Education Group")
plt.show()

```

