---
title: Assignment 03
author:
  - name: Taylor Luckenbill
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2025-09-22'

format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2
  docx:
    toc: true   # remove any reference-doc line if you don't have a template

date-modified: today
date-format: long
execute:
  echo: true
  eval: true
  freeze: auto
jupyter: python3
---


```{python}
#|echo: false
import pandas as pd
import plotly.express as px
import plotly.io as pio
from pyspark.sql import SparkSession
import re
import numpy as np
import plotly.graph_objects as go
from pyspark.sql.functions import col, split, explode, regexp_replace, transform, when
from pyspark.sql import functions as F
from pyspark.sql.functions import col, monotonically_increasing_id
```

```{python}
#|echo: false
np.random.seed(2)

pio.renderers.default = "notebook"

# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("data/lightcast_job_postings.csv")
df.createOrReplaceTempView("job_postings")

# Show Schema and Sample Data
#print("---This is Diagnostic check, No need to print it in the final doc---")

#df.printSchema() # comment this line when rendering the submission
df.show(5)
```
# Analysis Objective
## Data Preparation Activities
In order to complete the analysis the first step is to load and prepare the data. For this analysis I will update data types, calculate average salary, remove extraneous characters from relevant data text, and replace null values.

For the data set job_postings save as df and change the columns SALARY_FROM, SALARY_TO, SALARY, MIN_YEARS_EXPERIENCE, and MAX_YEARS_EXPERIENCE to type float.
```{python}
df = (
    df.withColumn("SALARY_FROM", col("SALARY_FROM").cast("float"))
      .withColumn("SALARY_TO", col("SALARY_TO").cast("float"))
      .withColumn("SALARY", col("SALARY").cast("float"))
      .withColumn("MIN_YEARS_EXPERIENCE", col("MIN_YEARS_EXPERIENCE").cast("float"))
      .withColumn("MAX_YEARS_EXPERIENCE", col("MAX_YEARS_EXPERIENCE").cast("float"))
)
```
Next the median needs to be calculated as it will be used to fill na values. Create a new method called compute_median. 
```{python}
def compute_median(sdf,col_name):
    q = sdf.approxQuantile(col_name,[0.5],0.01)
    return q[0] if q else None

median_from = compute_median(df,"SALARY_FROM")
median_to = compute_median(df, "SALARY_TO")
median_salary = compute_median(df, "SALARY")
```
```{python}
#|echo: false
print("medians: ", median_from, median_to, median_salary)
```
Next fill the missing values in the SALARY_FROM, SALARY_TO, SALARY with the median values.
```{python}
df = df.fillna({
    "SALARY_FROM": median_from,
    "SALARY_TO": median_to,
    "SALARY": median_salary})
```
Calculate average salary by adding the min and max salaries and dividing by 2.
```{python}
df = df.withColumn("Average Salary",(col("SALARY_FROM")+col("SALARY_TO"))/2)
```

```{python}
#|echo: false
print(df.columns) 
```
Next remove extraneous characters from education level names.
```{python}
from pyspark.sql.functions import regexp_replace, col

df = df.withColumn(
    "EDUCATION_LEVELS_NAME",
    regexp_replace(col("EDUCATION_LEVELS_NAME"), "[\n\r]", "")  
)
```
Select the columns to maintain in the analysis
```{python}
export_cols = [
    "EDUCATION_LEVELS_NAME",
    "REMOTE_TYPE_NAME",
    "MAX_YEARS_EXPERIENCE",
    "Average Salary",
    "LOT_V6_SPECIALIZED_OCCUPATION_NAME"
]


```
`confirmed selected columns`
```{python}
df_selected = df.select(export_cols)
df_selected.show(3)
```
Transform pyspark data into pandas dataframe from visualization.
```{python}
pdf = df_selected.toPandas()
pdf.to_csv("./data/lighthouse_cleaned.csv", index=False)
```
```{python}
#|echo: false
print(len(pdf))
```

```{python}
median_salaries = pdf.groupby("LOT_V6_SPECIALIZED_OCCUPATION_NAME")["Average Salary"].median()
```

```{python}
sorted_employment_types = median_salaries.sort_values(ascending=False).index
```

```{python}
pdf["LOT_V6_SPECIALIZED_OCCUPATION_NAME"] = pd.Categorical(
    pdf["LOT_V6_SPECIALIZED_OCCUPATION_NAME"],
    categories=sorted_employment_types,
    ordered=True
)
```

```{python}
fig = px.box(
    pdf,
    x="LOT_V6_SPECIALIZED_OCCUPATION_NAME",
    y="Average Salary"
)
```

```{python}
fig.show()
```

```{python}
print(df.columns) 
```

```{python}
from pyspark.sql.functions import lit

df = df.withColumn("counter", lit(1))
```

```{python}
#parse
export_cols2 = [
    "LOT_V6_SPECIALIZED_OCCUPATION_NAME",
    "Average Salary",
    "counter"
]
```

```{python}
df_selected2 = df.select(export_cols2)
df_selected2.show(40)
```

```{python}
pdf2 = df_selected2.toPandas()
#pdf2.to_csv("./data/lighthouse_cleaned.csv", index=False)
#print(len(pdf2))
pdf2.head(30)
```

```{python}
median_salaries2 = pdf2.groupby("LOT_V6_SPECIALIZED_OCCUPATION_NAME").agg({
    "Average Salary": "median",
    "counter": "sum"
}).reset_index()
```

```{python}
#sorted_employment_types2 = median_salaries2.sort_values(ascending=False).index
median_salaries2.head()
```

```{python}
fig2 = px.scatter(
    median_salaries2,
    x="LOT_V6_SPECIALIZED_OCCUPATION_NAME",
    y="Average Salary",
    size="counter",
    hover_name="LOT_V6_SPECIALIZED_OCCUPATION_NAME", 
    size_max=60       
)

fig2.update_layout(
    xaxis_title="Occupation",
    yaxis_title="Median Salary",
    title="Bubble Chart of Jobs: Median Salary vs. Occupation (Bubble = # Postings)",
    xaxis_tickangle=45              # rotate labels if long
)
```

```{python}
df = df.withColumn(
    "education_group",
    F.when(F.col("MIN_EDULEVELS_NAME").isin("GED", "Associate", "No Education Listed"), "Associate's or lower")
     .when(F.col("MIN_EDULEVELS_NAME") == "Bachelor's degree", "Bachelor's")
     .when(F.col("MIN_EDULEVELS_NAME").isin("Master's degree"), "Master's")
     .when(F.col("MIN_EDULEVELS_NAME").isin("PhD", "Doctorate", "professional degree"), "PhD")
     .otherwise("Other")  # optional catch-all for unexpected values
)

df.show()
```

```{python}
#parse
export_cols3 = [
    "LOT_V6_SPECIALIZED_OCCUPATION_NAME",
    "Average Salary",
    "MAX_YEARS_EXPERIENCE"
]
```

```{python}
df_selected3 = df.select(export_cols3)
df_selected3.show(10)
```

```{python}
pdf3 = df_selected3.toPandas()
#pdf2.to_csv("./data/lighthouse_cleaned.csv", index=False)
#print(len(pdf2))
pdf3.head(10)
```

```{python}
pdf3["MAX_YEARS_EXPERIENCE"] = pdf3["MAX_YEARS_EXPERIENCE"].fillna(0)
pdf3.head(10)
```

```{python}
import matplotlib.pyplot as plt
```

```{python}
groups = pdf3["LOT_V6_SPECIALIZED_OCCUPATION_NAME"].unique()

plt.figure(figsize=(10,6))

for g in groups:
    subset = pdf3[pdf3["LOT_V6_SPECIALIZED_OCCUPATION_NAME"] == g]
    
    # Add jitter to avoid overlapping points
    x_jitter = subset["MAX_YEARS_EXPERIENCE"] + np.random.normal(0, 0.2, size=len(subset))
    y_jitter = subset["Average Salary"] + np.random.normal(0, 0.2, size=len(subset))
    
    plt.scatter(x_jitter, y_jitter, alpha=0.6, label=g)

plt.xlabel("X-axis (with jitter)")
plt.ylabel("Y-axis (with jitter)")
plt.title("Scatter Plots per Group with Jitter")
plt.legend()
plt.show()
```

